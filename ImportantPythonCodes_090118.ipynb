{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Index of an item in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(df1).index('NoOfBirthDatesSearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a pandas column data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2[['Warning1']] = df2[['Warning1']].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a value in  a cell in pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.set_value(row_number, Column name, 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace a certain string with another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s.replace(\"[\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NA with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a column whicg got items like 101,102,103.. to put them in different columns and do one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "\n",
    "def one_hot_encode_tags_Income_Sources(df):\n",
    "    df.Income_Sources=df.Income_Sources.str.replace(\",\",\" \")\n",
    "    vect = CountVectorizer()\n",
    "    X = vect.fit_transform(df.Income_Sources)\n",
    "    df=df.join(pd.DataFrame(X.toarray(), columns=vect.get_feature_names()))\n",
    "    df.drop(\"Income_Sources\",axis=1,inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore warnings. Warning wont be shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = probxgbrf\n",
    "preds = probs \n",
    "fpr, tpr, threshold = metrics.roc_curve(test_y, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "fpr2, tpr2, threshold = metrics.roc_curve(test_y, prob_sc2)\n",
    "fpr3, tpr3, threshold = metrics.roc_curve(test_y, df_porb['probxgfinaprobxgbrf'])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'Final logistic model = %0.2f' % roc_auc_score(test_y, probxgbrf))\n",
    "plt.plot(fpr2,tpr2,label=\"Scorecard 2.0, auc=\"+str(roc_auc_score(test_y, prob_sc2)))\n",
    "plt.plot(fpr3,tpr3,label=\"Final XGBoost model, auc=\"+str(roc_auc_score(test_y, df_porb['probxgfinaprobxgbrf'])))\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform grid search  cross validation with the specified test set. tune parameters to optimize the performance of a specified test data set. To do a general cross validation change the value of CV parameter(Ex: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_test_fold = []\n",
    "\n",
    "# put -1 here, so they will be in training set\n",
    "for i in range(len(train_x)):\n",
    "    my_test_fold.append(-1)\n",
    "\n",
    "# for all greater indices, assign 0, so they will be put in test set\n",
    "for i in range(len(test_x)):\n",
    "    my_test_fold.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "param = {\n",
    "'n_estimators':[200,100,10],\n",
    "'max_depth':[5],\n",
    "'min_child_weight':[3],\n",
    "'reg_alpha':[6],\n",
    "'gamma':[0.6],\n",
    "'learning_rate':[0.09]\n",
    "}\n",
    "\n",
    "\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.md\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( \n",
    "        objective= 'reg:linear', \n",
    "        seed=1), \n",
    "    param_grid = param, \n",
    "    scoring='roc_auc',\n",
    "    cv = list(PredefinedSplit(test_fold=my_test_fold).split(new_data_df, df_y)),\n",
    "    verbose = 1)\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "# mean_squared_error alternative.\n",
    "\n",
    "gsearch1.fit(new_data_df, df_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive feature elimination method for feature selection. To do with other algorithms like xgboost, change the model at parameter 'estimator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr=LogisticRegression(C=0.8)\n",
    "\n",
    "rfe = RFE(estimator=lr, n_features_to_select=40, step=2)\n",
    "rfe.fit(new_data_df, df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining probabilities of multiple models. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probxgbrf=[(1*x + 1.6*y+0.6*z)/3.2\n",
    "           for x, y,z in zip(probxgb, probxgbf,problr)]\n",
    "roc_auc_score(test_y, probxgbrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a multi dimensional list and convert it to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list1=[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(0,len(df)):\n",
    "    x=df.iloc[i][1]\n",
    "    AdvisorLoanID=df.iloc[i][0]\n",
    "\n",
    "    crs=creditSearchLast2Weeks(x)\n",
    "    crsFN=creditSearchLast2WeeksOF(x)\n",
    "    csrBK=creditSearchLast2WeeksBK(x)\n",
    "    No_CA=NocurrentAccountsTOTAL(x)\n",
    "    Age_oldca=AgeOldestCurrentAccounts(x)\n",
    "    No_BDAYS=NoOfBirthDatesACC(x)\n",
    "    No_BDAYS_seearch=NoOfBirthDatesSearch(x)\n",
    "    \n",
    "    \n",
    "    list1[0].append(AdvisorLoanID)\n",
    "    list1[1].append(crs)\n",
    "    list1[2].append(crsFN)\n",
    "    list1[3].append(csrBK)\n",
    "    list1[4].append(No_CA)\n",
    "    list1[5].append(Age_oldca)\n",
    "    list1[6].append(No_BDAYS)\n",
    "    list1[7].append(No_BDAYS_seearch)\n",
    "    list1[8].append(TimeAfterTheLatestDefault(x))\n",
    "    list1[9].append(Telecom(x))\n",
    "    list1[10].append(creditlineCount(x))\n",
    "    list1[11].append(AgeOldestAccounts(x))\n",
    "    list1[12].append(timesBalExceedLimitRecent12AllAcc(x))\n",
    "    list1[13].append(AvgAmountBalExceedLimitRecent12AllAcc(x))\n",
    "    list1[14].append(MostRecentStutusOfDefaultedLoans(x))\n",
    "    list1[15].append(AvgAmountArrearsCreditcardsLast3month(x))\n",
    "    list1[16].append(AvgAmountArrearsCurrentACLast3month(x))\n",
    "    list1[17].append(AvgBalanceCurrentACLast3month(x))\n",
    "    list1[18].append(creditSearchLast2WeeksFN(x))\n",
    "    \n",
    "df1 = pd.DataFrame()\n",
    "\n",
    "df1['AdvisorLoanID']=list1[0]\n",
    "df1['creditSearchLast2Weeks']=list1[1]\n",
    "df1['creditSearchLast2WeeksOF']=list1[2]\n",
    "df1['creditSearchLast2WeeksBK']=list1[3]\n",
    "df1['NocurrentAccountsTOTAL']=list1[4]\n",
    "df1['AgeOldestCurrentAccounts']=list1[5]\n",
    "df1['NoOfBirthDatesACC']=list1[6]\n",
    "df1['NoOfBirthDatesSearch']=list1[7]\n",
    "df1['TimeAfterTheLatestDefault']=list1[8]\n",
    "df1['Telecom']=list1[9]\n",
    "df1['creditlineCount']=list1[10]\n",
    "df1['AgeOldestAccounts']=list1[11]\n",
    "df1['timesBalExceedLimitRecent12AllAcc']=list1[12]\n",
    "df1['AvgAmountBalExceedLimitRecent12AllAcc']=list1[13]\n",
    "df1['MostRecentStutusOfDefaultedLoans']=list1[14]\n",
    "df1['AvgAmountArrearsCreditcardsLast3month']=list1[15]\n",
    "df1['AvgAmountArrearsCurrentACLast3month']=list1[16]\n",
    "df1['AvgBalanceCurrentACLast3month']=list1[17]\n",
    "df1['creditSearchLast2WeeksFN']=list1[18]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Visualise a decision tree. First fit a tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "\n",
    "features=list(new_data_df)\n",
    "\n",
    "\n",
    "def get_code(tree, feature_names, target_names,\n",
    "             spacer_base=\"    \"):\n",
    "    \"\"\"Produce psuedo-code for decision tree.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-leant DescisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    target_names -- list of target (class) names.\n",
    "    spacer_base -- used for spacing code (default: \"    \").\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    based on http://stackoverflow.com/a/30104792.\n",
    "    \"\"\"\n",
    "    left      = tree.tree_.children_left\n",
    "    right     = tree.tree_.children_right\n",
    "    threshold = tree.tree_.threshold\n",
    "    features  = [feature_names[i] for i in tree.tree_.feature]\n",
    "    value = tree.tree_.value\n",
    "\n",
    "    def recurse(left, right, threshold, features, node, depth):\n",
    "        spacer = spacer_base * depth\n",
    "        if (threshold[node] != -2):\n",
    "            print(spacer + \"if ( \" + features[node] + \" <= \" + \\\n",
    "                  str(threshold[node]) + \" ) {\")\n",
    "            if left[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            left[node], depth+1)\n",
    "            print(spacer + \"}\\n\" + spacer +\"else {\")\n",
    "            if right[node] != -1:\n",
    "                    recurse(left, right, threshold, features,\n",
    "                            right[node], depth+1)\n",
    "            print(spacer + \"}\")\n",
    "        else:\n",
    "            target = value[node]\n",
    "            for i, v in zip(np.nonzero(target)[1],\n",
    "                            target[np.nonzero(target)]):\n",
    "                target_name = target_names[i]\n",
    "                target_count = int(v)\n",
    "                print(spacer + \"return \" + str(target_name) + \\\n",
    "                      \" ( \" + str(target_count) + \" examples )\")\n",
    "\n",
    "    recurse(left, right, threshold, features, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_code(dt, features, train_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pydotplus as pydot\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "\n",
    "from sklearn.externals.six import StringIO\n",
    "\n",
    "dot_data = StringIO()\n",
    "\n",
    "tree.export_graphviz(dt, out_file=dot_data,feature_names=features,filled=True, proportion=True)\n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save multiple dataframes in one excel file in different sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('pandas_multiple.xlsx', engine='xlsxwriter')\n",
    "\n",
    "df1.to_excel(writer, sheet_name='Sheet1')\n",
    "df2.to_excel(writer, sheet_name='Sheet2')\n",
    "df3.to_excel(writer, sheet_name='Sheet3')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query SQL with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "\n",
    "query = \" select left(PeriodEndDateKey, 6) as PeriodEndDate,LoanAmountTaken,CapitalBalanceWrittenOff, left(LoanStartDateKey, 6) as LoanStartDate, LSSLoanID \\\n",
    "FROM dw..FactMonthlyArchiveSnapshot \\\n",
    "where left(LoanStartDateKey, 6) >= 201701 and left(LoanStartDateKey, 6) <= 201703 \"\n",
    "\n",
    "\n",
    "engine_str = \"mssql+pyodbc://@TCSVVW8PSQL005\\SQL005/risk?driver=SQL Server\"\n",
    "\n",
    "engine = sqlalchemy.create_engine(engine_str)\n",
    "df = pd.read_sql(query, engine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot multiple graphs in one plot and set figure size and display legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in all_origin_months:\n",
    "    \n",
    "    get_cumWO(i)['%WO'].plot('line',label=i)  #plot for all the years in a loop\n",
    "    \n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18, 12) #  change the plot size\n",
    "#fig.savefig('test2png.png', dpi=100)   \n",
    "\n",
    "plt.xlabel('Months_On_Books')\n",
    "plt.ylabel('%W0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a string a global variable. This wat we can create variables dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "globals()['df_M'+str(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a column pandas based on location. Following code will select column index number 2. note that Indexing start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.ix[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is a column with strings seperated by / ex: aaa/bbb/ccc if we want to put each string in to different columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(txt):\n",
    "    try :\n",
    "        return txt.split(\"/\")\n",
    "    except :\n",
    "        return (\"No Label\", \"No Label\", \"No Label\") \n",
    "\n",
    "train_test['general_category'],train_test['subcategory_1'],train_test['subcategory_2'] = \\\n",
    "zip(*train_test['category_name'].apply(lambda x: split(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column named 'GC' and fill that column with specified values conditioned on another column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['GC']=0\n",
    "train_test.loc[ (train_test.general_category=='Beauty'),'GC']=19.671536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the number of unique items in a column pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test['brand_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using countvectorizer or tfidvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = CountVectorizer(min_df=NAME_MIN_DF)\n",
    "X_name = count.fit_transform(df[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all the values of a dataframe to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting with matplotlib pylplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "(train['price']).plot.hist(bins=50, figsize=(20,10), edgecolor='white',range=[0,250])\n",
    "plt.xlabel('price+', fontsize=17)\n",
    "plt.ylabel('frequency', fontsize=17)\n",
    "plt.tick_params(labelsize=15)\n",
    "plt.title('Price Distribution - Training Set', fontsize=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a value between strings which is computed in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"There are %d items that do not have a label.\" % train['category_name'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploring dataframe varibales- frequency plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].hist(df1.alcohol, 10, facecolor='red', alpha=0.5, label=\"Red wine\")\n",
    "ax[1].hist(df2.alcohol, 10, facecolor='white', ec=\"black\", lw=0.5, alpha=0.5, label=\"White wine\")\n",
    "\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=1)\n",
    "ax[0].set_ylim([0, 1000])\n",
    "ax[0].set_xlabel(\"Alcohol in % Vol\")\n",
    "ax[0].set_ylabel(\"Frequency\")\n",
    "ax[1].set_xlabel(\"Alcohol in % Vol\")\n",
    "ax[1].set_ylabel(\"Frequency\")\n",
    "#ax[0].legend(loc='best')\n",
    "#ax[1].legend(loc='best')\n",
    "fig.suptitle(\"Distribution of Alcohol in % Vol\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks with Keras calculates AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_proba(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            logging.info(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))\n",
    "\n",
    "# (snip)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "    #train_x_array, train_y_array, test_x_array, test_y_array = load_data()\n",
    "    ival = IntervalEvaluation(validation_data=(test_x_array,test_y_array), interval=1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(h1, activation='relu', input_dim=len(train_x_array[0]), kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros')) #2\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(h2, activation='relu',kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros')) # 2\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(h3, activation='relu',kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros')) # 2\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(2, activation='softmax',kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros'))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "    \n",
    "    \n",
    "    model.fit(train_x_array, train_y_array, nb_epoch=20, batch_size=30, verbose=0, callbacks=[ival])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate AUC,GINI of test set in every epoch and save the checkpoint in each epoch if there is an improvement from the previous epoch. Each check point is saved with different file names with epoch number and the auc in the val set. So later we can pick the best checkpoint based on all the checkpoints auc. If you need to save only the best model make the filename fixed so each time checkpoint file is overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class roc_auc_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict_proba(self.x, verbose=0)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        logs['roc_auc'] = roc_auc_score(self.y, y_pred)\n",
    "        logs['norm_gini'] = ( roc_auc_score(self.y, y_pred) * 2 ) - 1\n",
    "\n",
    "        y_pred_val = self.model.predict_proba(self.x_val, verbose=0)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['norm_gini_val'] = ( roc_auc_score(self.y_val, y_pred_val) * 2 ) - 1\n",
    "\n",
    "        print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round((roc*2-1),5)),str(round((roc_val*2-1),5))), end=10*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(h1, activation='relu', input_dim=len(train_x_array[0]), kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros')) #2\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(h2, activation='relu',kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros')) # 2\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(h3, activation='relu',kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros')) # 2\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(2, activation='softmax',kernel_initializer='random_uniform',use_bias=True,bias_initializer='zeros'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[metrics.categorical_accuracy])\n",
    "\n",
    "# Generate dummy data\n",
    "\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "filepath=\"weights-improvement-{epoch:02d}-{roc_auc_val:.5f}.hdf5\"\n",
    "callbacks = [\n",
    "            roc_auc_callback(training_data=(train_x_array, train_y_array),validation_data=(test_x_array, test_y_array)),  # call this before EarlyStopping\n",
    "            EarlyStopping(monitor='norm_gini_val', patience=10, mode='max', verbose=0),\n",
    "            CSVLogger('keras-epochs.log', separator=',', append=False),\n",
    "            ModelCheckpoint(filepath\n",
    "                    ,\n",
    "                    monitor='norm_gini_val', mode='max', # mode must be set to max or Keras will be confused\n",
    "                    save_best_only=True,\n",
    "                    verbose=1)\n",
    "        ]\n",
    "        \n",
    "#'keras--run-01-v1-fold-' +'-run-'+'.check'\n",
    "#filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_x_array, train_y_array, epochs=20, batch_size=30, callbacks=callbacks, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "#history=model.fit(train_x_array, train_y_array, epochs=20, batch_size=30)\n",
    "\n",
    "#plt.plot(history.history['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all na rows of a specific col name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1[np.isfinite(df1['NocurrentAccountsTOTAL'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Renaming columns when there are duplicated column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols=pd.Series(df.columns)\n",
    "for dup in df.columns.get_duplicates(): cols[df.columns.get_loc(dup)]=[dup+'.'+str(d_idx) if d_idx!=0 else dup for d_idx in range(df.columns.get_loc(dup).sum())]\n",
    "df.columns=cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot with pyplot and save the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(df,list2,title,a,b):\n",
    "    \n",
    "    d1=df.T\n",
    "\n",
    "\n",
    "    d1['index1'] = d1.index\n",
    "\n",
    "    list1=[]\n",
    "    for i in range(0,len(d1['index1'].tolist())):\n",
    "        list1.append(i)\n",
    "    LABELS=list2\n",
    "    #LABELS=list(d1['index1'])\n",
    "    fig = plt.figure(1, figsize=(a, b)) \n",
    "\n",
    "    plt.bar(list1, d1[0].tolist(),color='c',width=0.5)\n",
    "    plt.xticks(list1, LABELS,rotation='vertical')\n",
    "    \n",
    "    return fig.savefig(title,bbox_inches = 'tight')   ## using bbox_inches = 'tight' will not crop when you save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas bar plot add x y labels and change label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax=(df1['timehoursAPPbin'].value_counts()/df1['timehoursAPPbin'].value_counts().sum()).sort_index().plot.bar(figsize=(12, 6))\n",
    "ax.set_xlabel(\"time for processing applications in hours\")\n",
    "\n",
    "# Set the y-axis label\n",
    "ax.set_ylabel(\"% Apps\")     \n",
    "\n",
    "ax.set_xticklabels(['0-5','5-10','10-15','15-20','20-50','50-100','100+'], rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple condition pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ret=df_ret[(df_ret['max']!='SA') & (df_ret['max']!='SD') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop rows where NA s in a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df.dropna(subset=['JourneyType']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "#from bokeh.transform import factor_cmap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordCount(text):\n",
    "    # convert to lower case and strip regex\n",
    "    try:\n",
    "         # convert to lower case and strip regex\n",
    "        text = text.lower()\n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        txt = regex.sub(\" \", text)\n",
    "        # tokenize\n",
    "        # words = nltk.word_tokenize(clean_txt)\n",
    "        # remove words in stop words\n",
    "        words = [w for w in txt.split(\" \") \\\n",
    "                 if not w in stop_words.ENGLISH_STOP_WORDS and len(w)>3]\n",
    "        return len(words)\n",
    "    except: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    sent_tokenize(): segment text into sentences\n",
    "    word_tokenize(): break sentences into words\n",
    "    \"\"\"\n",
    "    try: \n",
    "        regex = re.compile('[' +re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        \n",
    "        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n",
    "        tokens = []\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n",
    "        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n",
    "        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n",
    "        \n",
    "        return filtered_tokens\n",
    "            \n",
    "    except TypeError as e: print(text,e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['tokens'] = train['item_description'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_wordcloud(tup):\n",
    "    wordcloud = WordCloud(background_color='white',\n",
    "                          max_words=50, max_font_size=40,\n",
    "                          random_state=42\n",
    "                         ).generate(str(tup))\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.imshow(generate_wordcloud(women100), interpolation=\"bilinear\")\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Women Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.imshow(generate_wordcloud(beauty100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Beauty Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.imshow(generate_wordcloud(kids100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Kids Top 100\", fontsize=30)\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.imshow(generate_wordcloud(electronics100))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Electronic Top 100\", fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convert ratio to a percentage when plotting with pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "def to_percent(y, position):\n",
    "    # Ignore the passed in position. This has the effect of scaling the default\n",
    "    # tick locations.\n",
    "    s = str(100 * round(y,2))\n",
    "\n",
    "    # The percent symbol needs escaping in latex\n",
    "    if matplotlib.rcParams['text.usetex'] is True:\n",
    "        return s + r'$\\%$'\n",
    "    else:\n",
    "        return s + '%'\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot(df,list2,title,a,b):\n",
    "    \n",
    "    d1=df.T\n",
    "\n",
    "\n",
    "    d1['index1'] = d1.index\n",
    "\n",
    "    list1=[]\n",
    "    for i in range(0,len(d1['index1'].tolist())):\n",
    "        list1.append(i)\n",
    "    LABELS=list2\n",
    "    #LABELS=list(d1['index1'])\n",
    "    fig = plt.figure(1, figsize=(a, b)) \n",
    "\n",
    "    plt.bar(list1, d1[0].tolist(),color='c',width=0.5)\n",
    "    plt.xticks(list1, LABELS,rotation='vertical')\n",
    "    formatter = FuncFormatter(to_percent)\n",
    "\n",
    "# Set the formatter\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    return fig.savefig(title,bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### put mulitple returns to multiple columns in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First put multiple return in to a tuple and put in to one column and then split in to multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(df,users,start_MsgId,conv_duration_thres):\n",
    "    \n",
    "    df2=filter_conversation(df,users,start_MsgId)\n",
    "    dest_user=df2.iloc[0][3]\n",
    "    \n",
    "    try:\n",
    "        df3=df2[df2['user_id']==dest_user].reset_index(drop=True)\n",
    "\n",
    "        time_reply= df3.iloc[0][12]\n",
    "    except:\n",
    "        time_reply= 1000\n",
    "        \n",
    "    if time_reply < 300:\n",
    "        \n",
    "        \n",
    "        #dest_user=df2.iloc[0][3]\n",
    "        start_user=df2.iloc[0][1]\n",
    "        #df4=df2[df2['user_id']==dest_user].reset_index(drop=True)\n",
    "        df4=df3[df3['durationFromStartConv']< time_reply + conv_duration_thres].reset_index(drop=True)\n",
    "        df5=df2[df2['user_id']==start_user].reset_index(drop=True)\n",
    "        df5 = df5.iloc[1:]\n",
    "        df5=df5[df5['durationFromStartConv']< time_reply + conv_duration_thres].reset_index(drop=True)\n",
    "        reply_by_dest=df4['message2'].tolist()\n",
    "        reply_by_starter=df5['message2'].tolist()\n",
    "    else:\n",
    "        reply_by_dest= 'NA'\n",
    "        reply_by_starter='NA'\n",
    "        \n",
    "        \n",
    "    return (time_reply,reply_by_dest,reply_by_starter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1_ques['all']= df1_ques.apply(lambda row:run_all(df1,row['users'],row['MsgId'],15), axis=1)\n",
    "\n",
    "\n",
    "df1_ques[['time_reply', 'replybydest','replybysender']] = df1_ques['all'].apply(pd.Series)\n",
    "\n",
    "del df1_ques['all']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure time taken to run a code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "## CODE\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace value of a cell conditioned on other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_seg_id.profit < 0\n",
    "column_name = 'outcome'\n",
    "df_seg_id.loc[mask, column_name] = 'SD_R'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter rows with no missing values in a paritcular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_all[X_final_all.Thrill_Adventure_Seeking.notnull()]\n",
    "X_final_all[X_final_all.Thrill_Adventure_Seeking.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if a column contains a certian character , if so replace them with another value. nan in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in cont_feat_list:\n",
    "    df1.loc[df1[f].isin([\"C\",\"M\",\"H\",\"T\",\"__\",\"\",\"P\",\"E\",\"F\",\"G\",\"K\",\"I\"]),f] = np.nan\n",
    "    df1[f] = df1[f].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create new column conidioned on another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['color'] = np.where(df['Set']=='Z', 'green', 'red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### replace a certain string with nan. function applymap applies the function to every  cell in the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psyc_score = df_psyc_score.applymap(lambda x: np.nan if x== 'NA' else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin numerical values custom ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.IntervalIndex.from_tuples([(0, 1), (1, 4), (4, 8),(8,10000)])\n",
    "test_feat['test_feat_bin']=pd.cut(test_feat[feat], bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feat['test_feat_bin'] =pd.cut(test_feat[feat],i,right=False, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by  a col and take the averge of another col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cluster')['time'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  having a variable in sql query. For each type of data different character fromat should be used. here it is tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT *  FROM [Staging].[dbo].[CreditBureau_CreditPaymentHistory] where InsightId in %s and AgeInMonths <=6\" % (ins,)\n",
    "engine_str = \"mssql+pyodbc://@TCSVVW8PSQL005\\SQL005/risk?driver=SQL Server\"\n",
    "engine = sqlalchemy.create_engine(engine_str)\n",
    "df_payhist = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sql query in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in LoanID_list:\n",
    "    print(i)\n",
    "    query='''select *  FROM [Staging].[dbo].[LSSLive_LSSLoanTransaction]  \n",
    "    where LoanId = %d AND TransactionTypeID not in (6,5,18,24)\n",
    "    '''% (i,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert a binary categorical feature to 0 or one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = preprocessing.LabelBinarizer()\n",
    "#df_psyc_score['Thrill_Adventure_Seeking_missing']=lb.fit_transform(df_psyc_score['Thrill_Adventure_Seeking_missing'])\n",
    "df_psyc_score['Extraversion_Measure2_missing'] =lb.fit_transform(df_psyc_score['Extraversion_Measure2_missing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booleans = []\n",
    "for start in df2.Type:\n",
    "    if start == 'Cont' :\n",
    "            booleans.append(True)\n",
    "    else:\n",
    "        booleans.append(False)\n",
    "        \n",
    "Specifichour = pd.Series(booleans)\n",
    "\n",
    "df3=df2[Specifichour]\n",
    "df3=df3.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter dataframe rows if value in column is in a set list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insights2=df_insights[df_insights['AddressId'].isin(adressIDs_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frequency plot with seaborn and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = pyplot.subplots(figsize=(25,10))\n",
    "sns.countplot(ax=ax, data=df_COC2,x=\"COC\" )\n",
    "fig.savefig(\"COC_freq_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### group by a column and get the count of another column used for stacked bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df3.groupby(['COC', 'ThreeMonth_pay_ratio'])['ThreeMonth_pay_ratio'].count().unstack('ThreeMonth_pay_ratio').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacked bar plot/ change axis labels and fonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt=df5.plot(kind='bar', stacked=True,figsize=(25, 14),color=colormap,title='COC vs 3month_payment_ratio counts',fontsize=12)\n",
    "\n",
    "\n",
    "plt.set_title('COC vs 3month_payment_ratio counts', fontsize=20)\n",
    "plt.set_ylabel(\"Three month payment ratio distribution\",fontsize=16)\n",
    "plt.set_xlabel(\"COC\",fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colors when plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#194196\", \"#d27c88\", \"#36422b\", \"#b68f79\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing nans from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[1,float('NaN'),6,float('NaN')]\n",
    "l=[x for x in l if not math.isnan(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert a date string to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "def get_date_time (date1):\n",
    "    \n",
    "    year=str(date1)[:4]\n",
    "    mon=str(date1)[4:6]\n",
    "    day=str(date1)[6:8]\n",
    "    \n",
    "    a = (mon,)\n",
    "\n",
    "    dt1= a + (day,) + (year,)\n",
    "    s = \" \"\n",
    "    dt=s.join(dt1)\n",
    "    dt = parser.parse(dt)\n",
    "    \n",
    "    return dt\n",
    "    \n",
    "    \n",
    "df['datetime'] = df['PeriodEndDateID'].map(get_date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string list to a list and one hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "df_comp=pd.read_csv('no_unique_dob_companytype.csv')\n",
    "df_comp[['LongApplicationId']] = df_comp[['LongApplicationId']].astype(int)\n",
    "\n",
    "df_comp=df_comp[['company_type','LongApplicationId']].set_index('LongApplicationId')\n",
    "\n",
    "import numpy as np\n",
    "def company_type(l):\n",
    "    \n",
    "    x = l\n",
    "    try:\n",
    "        x = ast.literal_eval(x)\n",
    "\n",
    "\n",
    "        x = [n.strip() for n in x]\n",
    "\n",
    "        if len(x) >= 1:\n",
    "            \n",
    "\n",
    "            return (x)\n",
    "        else:\n",
    "            return ['ND']\n",
    "    except:\n",
    "        \n",
    "       \n",
    "        return ['ND']\n",
    "        \n",
    "        \n",
    "df_comp['company_type'] = df_comp['company_type'].map(company_type)\n",
    "df_comp = df_comp['company_type'].str.join('|').str.get_dummies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
